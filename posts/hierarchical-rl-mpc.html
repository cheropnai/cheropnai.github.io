<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Hierarchical RL-MPC — complexity.blog</title>
  <link rel="stylesheet" href="../assets/css/style.css">
</head>
<body>

  <!-- Nav -->
  <nav class="site-nav">
    <div class="container">
      <a href="../index.html" class="site-logo">complexity<span class="dot">.</span>blog</a>
      <ul class="nav-links">
        <li><a href="../index.html">Writing</a></li>
        <li><a href="../index.html#projects">Projects</a></li>
        <li><a href="https://github.com/YOUR_USERNAME" target="_blank">GitHub</a></li>
      </ul>
    </div>
  </nav>

  <!-- Post Header -->
  <header class="post-header">
    <div class="container">
      <a href="../index.html" class="back-link">&larr; Back to all posts</a>
      <h1>Hierarchical RL-MPC: Bridging Learning and Control</h1>
      <div class="post-date">February 18, 2026 &middot; 8 min read</div>
      <div style="margin-top: 0.75rem;">
        <span class="post-tag">rl</span>
        <span class="post-tag">control-theory</span>
        <span class="post-tag">robotics</span>
      </div>
    </div>
  </header>

  <!-- Post Body -->
  <article class="container">
    <div class="prose">

      <p>The gap between learning-based methods and classical control has always fascinated me. Reinforcement learning excels at discovering strategies in complex, uncertain environments. Model predictive control provides guarantees and precision. The question that drives this work: what happens when you make them talk to each other?</p>

      <h2>The Architecture</h2>

      <p>The core idea is a two-level hierarchy. At the top, an RL agent reasons about high-level goals — which room to visit next, which corridor to take, how to handle a blocked path. At the bottom, an MPC controller tracks the reference trajectory with the kind of tight, constraint-respecting control that RL alone struggles with.</p>

      <p>This separation isn't just architectural convenience. It reflects something real about how complex tasks decompose: strategic decisions happen at one timescale, motor control at another.</p>

      <pre><code># Pseudostructure of the hierarchy
class HighLevelAgent:
    """RL agent that outputs waypoints or subgoals."""
    def select_action(self, state, preferences):
        # Encodes learned human preferences
        # Returns next waypoint for the MPC layer
        ...

class LowLevelController:
    """MPC that tracks the RL agent's waypoints."""
    def solve(self, current_state, target_waypoint):
        # Quadratic program with dynamics constraints
        # Returns control inputs (velocity, steering)
        ...</code></pre>

      <h2>Where It Breaks</h2>

      <p>The first thing that breaks is the interface between layers. If the RL agent proposes waypoints that the MPC cannot physically reach within its horizon, the QP solver fails. I spent weeks debugging infeasible solutions before realizing the issue was upstream — the high-level agent had no model of what the low-level controller could actually do.</p>

      <p>The fix involved giving the RL agent a simplified feasibility check: a rough envelope of reachable states that filters out impossible subgoals before they reach the MPC. Not elegant, but effective.</p>

      <h3>Preference Learning Complications</h3>

      <p>Adding human preferences into the mix introduces another layer of difficulty. The reward signal isn't just "reach the goal" — it's "reach the goal the way the human prefers." This means learning from pairwise comparisons, which introduces noise, inconsistency, and all the usual problems of learning from humans.</p>

      <blockquote>
        The real challenge isn't optimizing the reward function. It's figuring out what the reward function should be in the first place.
      </blockquote>

      <h2>What I'm Exploring Next</h2>

      <p>Two directions feel promising. First, <strong>fuzzy state abstraction</strong> — rather than hard-coding discrete states (corridor, room, junction), allowing the boundaries to be soft and learnable. Second, <strong>transition model learning</strong> — letting the high-level agent learn its own dynamics model of how states transition, rather than relying on a hand-designed graph.</p>

      <p>Both of these ideas push toward a system that's more adaptive and less brittle. Whether they actually work in practice is the subject of the next few posts.</p>

      <hr style="border: none; border-top: 1px solid var(--border); margin: 3rem 0;">

      <p style="color: var(--text-muted); font-size: 0.9rem;">This post is part of an ongoing series on hierarchical decision systems. If you're working on similar problems, I'd love to hear about it.</p>

    </div>
  </article>

  <!-- Footer -->
  <footer class="site-footer">
    <div class="container">
      <p>&copy; 2026 complexity.blog</p>
      <ul class="footer-links">
        <li><a href="https://github.com/YOUR_USERNAME" target="_blank">GitHub</a></li>
        <li><a href="#">RSS</a></li>
      </ul>
    </div>
  </footer>

</body>
</html>
